{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "vocab_size = 74072 + 1\n",
    "output_size = 2\n",
    "embedding_dim =  400\n",
    "hidden_dim = 512 \n",
    "n_layers = 2        \n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "net = torch.load('movie_review_sentiment.pth', map_location=lambda storage, loc: storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_to_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m test_review_neg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mThe worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39m# test code and generate tokenized review\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m test_ints \u001b[39m=\u001b[39m tokenize_review(test_review_neg)\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(test_ints)\n\u001b[0;32m     24\u001b[0m \u001b[39m# test sequence padding\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mtokenize_review\u001b[1;34m(test_review)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# tokens\u001b[39;00m\n\u001b[0;32m     12\u001b[0m test_ints \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m test_ints\u001b[39m.\u001b[39mappend([vocab_to_int[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m test_words])\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m test_ints\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# tokens\u001b[39;00m\n\u001b[0;32m     12\u001b[0m test_ints \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m test_ints\u001b[39m.\u001b[39mappend([vocab_to_int[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m test_words])\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m test_ints\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_to_int' is not defined"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)\n",
    "\n",
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "print(features)\n",
    "\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
